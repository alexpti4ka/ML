{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import PyPDF2\n",
    "from transformers import pipeline\n",
    "from pathlib import Path\n",
    "import os\n",
    "from transformers import T5Tokenizer, TFT5ForConditionalGeneration\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to Hugging Face\n",
    "#login(token=\"\") from env!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from a single PDF file and save to a Markdown file\n",
    "def extract_text_from_pdf():\n",
    "    with open(pdf_path, \"rb\") as pdf_file:\n",
    "        reader = PyPDF2.PdfReader(pdf_file)\n",
    "        text = \"\"\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            page_text = page.extract_text()\n",
    "            text += f\"# Page {i + 1}\\n\\n\" + page_text + \"\\n\\n\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"/Users/alexpti4ka/Documents/Books/ВКР/logiko-semanticheskaya-model-informatsionno-spravochnyh-sistem-v-gosudarstvennom-upravlenii.pdf\"\n",
    "output_path = \"/Users/alexpti4ka/Documents/Books/ВКР/Texts_from_pdf/extracted_text.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text saved to /Users/alexpti4ka/Documents/Books/ВКР/Texts_from_pdf/extracted_text.txt\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    extracted_text = extract_text_from_pdf()\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as md_file:\n",
    "        md_file.write(extracted_text)\n",
    "    print(f\"Extracted text saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_path = \"/Users/alexpti4ka/Documents/Books/ВКР/podhody-k-sozdaniyu-ontologiy-dlya-avtomatizirovannyh-sistem-v-mashinostroitelnyh-proizvodstvah.pdf\"\n",
    "# if __name__ == \"__main__\":\n",
    "#     extracted_text = extract_text_from_pdf()\n",
    "#     print(extracted_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7c628d944a42b480ca335439307cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/20.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01399a7a44c473586c7633a6b8f96a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/1.47M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ffedb915ac746d7ac1e38fc74b792fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33d0b2bf151485eac55530fea7022ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cuda'  # or 'cpu' for translate on cpu\n",
    "model_name = 'utrobinmv/t5_summary_en_ru_zh_base_2048'\n",
    "model = TFT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to summarize text using the model\n",
    "def summarize_text(text, prefix=\"summary big: \"):\n",
    "    src_text = prefix + text\n",
    "    input_ids = tokenizer(src_text, return_tensors=\"tf\")['input_ids']\n",
    "    generated_tokens = model.generate(input_ids, max_length=1000)\n",
    "    result = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read extracted text from file and summarize it\n",
    "def summarize_from_file():\n",
    "    with open(output_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        document_text = file.read()\n",
    "    summarized_text = summarize_text(document_text)\n",
    "    print(\"Summarized Text:\\n\")\n",
    "    print(summarized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the type of summary: \"summary: \", \"summary brief: \", \"summary big: \"\n",
    "summary_prefix = \"summary big: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized Text:\n",
      "\n",
      "В 2020-305 годах разработчики машиностроительных предприятий, выпускающих наукоем-про-дукцию, представили проект онтологов для создания автоматических систем. В 2020-305 годах онтологии, разработанного на основе онтологов, и онтологов, которые, как предполагается, имеют нужную структуру, могут быть использованы для создания онтологов.\n"
     ]
    }
   ],
   "source": [
    "summarize_from_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from a single PDF file and split into paragraphs\n",
    "def extract_text_to_json():\n",
    "    with open(pdf_path, \"rb\") as pdf_file:\n",
    "        reader = PyPDF2.PdfReader(pdf_file)\n",
    "        paragraphs = []\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            page_paragraphs = page_text.split(\"\\n\\n\")  # Split by double newlines for paragraphs\n",
    "            paragraphs.extend([para.strip() for para in page_paragraphs if para.strip()])\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(paragraphs, json_file, ensure_ascii=False, indent=4)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to summarize a single paragraph using the model\n",
    "def summarize_paragraph(paragraph, prefix=summary_prefix):\n",
    "    src_text = prefix + paragraph\n",
    "    input_ids = tokenizer(src_text, return_tensors=\"tf\")['input_ids']\n",
    "    generated_tokens = model.generate(input_ids, max_length=512)\n",
    "    result = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to summarize all paragraphs from a JSON file\n",
    "def summarize_from_json():\n",
    "    with open(output_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        paragraphs = json.load(json_file)\n",
    "    summarized_paragraphs = [summarize_paragraph(para) for para in paragraphs]\n",
    "    for i, summary in enumerate(summarized_paragraphs):\n",
    "        print(f\"Paragraph {i + 1} Summary:\\n{summary}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/alexpti4ka/Documents/Books/ВКР/Texts_from_pdf/extracted_text.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m summarize_from_json()\n",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m, in \u001b[0;36msummarize_from_json\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msummarize_from_json\u001b[39m():\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[1;32m      4\u001b[0m         paragraphs \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(json_file)\n\u001b[1;32m      5\u001b[0m     summarized_paragraphs \u001b[38;5;241m=\u001b[39m [summarize_paragraph(para) \u001b[38;5;28;01mfor\u001b[39;00m para \u001b[38;5;129;01min\u001b[39;00m paragraphs]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/alexpti4ka/Documents/Books/ВКР/Texts_from_pdf/extracted_text.json'"
     ]
    }
   ],
   "source": [
    "summarize_from_json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
